import torch
import pandas as pd
import numpy as np
import os
import json
import ast
from torch.utils.data import DataLoader, Subset

from models.encdec_model import EncDecModel 

from src.dataset_module import SolarEfficientDataset
from src.preprocessing import SolarPreprocessor
from utils.xai import SolarXAIEngine

# ================= CONFIGURA√á√ÉO =================
CSV_PATH = 'data/pv0.csv' # Certifique-se que este √© o arquivo limpo
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

EXPERIMENTS_DIRS = [
    "experiments/2025-12-19_19-09-00_EDLSTM_PAZTUVD"
]

OUTPUT_DIR = "experiments/Analise_XAI_Profunda"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ================= FUN√á√ïES AUXILIARES =================

def safe_parse_config(config):
    parsed = config.copy()
    for key in ['hidden_sizes', 'feature_cols']:
        if key in parsed and isinstance(parsed[key], str):
            try: parsed[key] = ast.literal_eval(parsed[key])
            except: pass 
    for key in ['dropout', 'learning_rate', 'dropout_prob']:
        if key in parsed: parsed[key] = float(parsed[key])
    for key in ['input_seq_len', 'output_seq_len', 'input_size', 'epochs']:
        if key in parsed: parsed[key] = int(parsed[key])
    for key in ['bidirectional', 'use_attention', 'use_feature_attention']:
        if key in parsed:
            val = parsed[key]
            parsed[key] = val.lower() == 'true' if isinstance(val, str) else bool(val)
    return parsed

def load_config_and_model(exp_dir):
    config_path = os.path.join(exp_dir, 'config.json')
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"config.json n√£o encontrado em {exp_dir}")

    with open(config_path, 'r') as f:
        raw_config = json.load(f)
    
    config = safe_parse_config(raw_config)
    
    feature_cols = config.get('feature_cols', [])
    
    model = EncDecModel(
        input_size=len(feature_cols),
        hidden_sizes=config['hidden_sizes'],
        output_seq_len=config['output_seq_len'],
        output_dim=1,
        cell_type=config.get('cell_type', 'lstm'),
        bidirectional=config.get('bidirectional', False),
        use_attention=config.get('use_attention', False),
        use_feature_attention=config.get('use_feature_attention', False)
    ).to(DEVICE)
    
    weights_path = os.path.join(exp_dir, 'best_model.pth') 
    if not os.path.exists(weights_path):
        weights_path = os.path.join(exp_dir, 'best_model.pt') 
        
    model.load_state_dict(torch.load(weights_path, map_location=DEVICE))
    model.eval()
    
    return model, config

def load_data(csv_path, feature_cols, n_past, n_future):
    print("‚è≥ Carregando e limpando dados...")
    df = pd.read_csv(csv_path)
    
    # --- LIMPEZA AGRESSIVA DE DADOS (Igual ao script fix) ---
    if 'Date_Time' in df.columns:
        df['Date_Time'] = pd.to_datetime(df['Date_Time'])
        # Agrupa por data e pega o primeiro (Remove duplicatas for√ßadamente)
        df = df.groupby('Date_Time').first().reset_index()
        df = df.set_index('Date_Time').sort_index()
    
    # Garante unicidade do √≠ndice
    df = df[~df.index.duplicated(keep='first')]
    # ----------------------------------
    
    if 'Year' not in df.columns: df['Year'] = df.index.year
    # Filtro de ano (Opcional: ajuste conforme sua necessidade)
    # df_test = df[df['Year'] == 2022].copy()
    df_test = df.copy() # Usando todo o dataset por enquanto
    
    preprocessor = SolarPreprocessor(
        latitude=-15.60, 
        longitude=-47.70,
        timezone='Etc/GMT+3',
        nominal_power=156.0,
        target_col='Pot_BT',
        start_year=2018,
        column_mapping={
            'Pot_BT': 'target',
            'Irradia√ß√£o Global hor√°ria(horizontal) kWh/m2': 'ghi',
            'Irradia√ß√£o Difusa hor√°ria kWh/m2': 'dhi',
            'Irradia√ß√£o Global hor√°ria(Inclinada 27¬∞) kWh/m2': 'irrad_poa',
            'Temperatura ambiente ¬∞C': 'temp_amb',
            'Umidade Relativa %': 'humidity',
            'Velocidade m√©dia do vento m/s': 'wind_speed'
        }
    )
    
    # --- CORRE√á√ÉO: CARREGA SCALERS DA RAIZ ---
    SCALER_DIR = "." # Procura no diret√≥rio atual
    try:
        print(f"‚ôªÔ∏è Carregando scalers de: {os.path.abspath(SCALER_DIR)}")
        preprocessor.load_scalers(SCALER_DIR)
    except Exception as e:
        print(f"‚ùå ERRO CR√çTICO: N√£o encontrei scalers em {SCALER_DIR}.")
        raise RuntimeError("Imposs√≠vel rodar XAI sem os scalers originais (scaler_X.pkl, scaler_Y.pkl). Copie-os para a pasta raiz.")
        
    df_proc = preprocessor.transform(df_test)
    
    valid_cols = [c for c in feature_cols if c in df_proc.columns]
    
    dataset = SolarEfficientDataset(
        df_proc, 
        input_tag=valid_cols, 
        n_past=n_past, 
        n_future=n_future,
    )
    return dataset, df_proc

# ================= MAIN XAI PIPELINE =================
def main():
    print(f"\nüöÄ Iniciando An√°lise XAI em {len(EXPERIMENTS_DIRS)} modelos...")
    
    for exp_dir in EXPERIMENTS_DIRS:
        if not os.path.exists(exp_dir): continue

        model_name = os.path.basename(exp_dir)
        print(f"\nüî¨ Analisando: {model_name}")
        
        save_folder = os.path.join(OUTPUT_DIR, model_name)
        os.makedirs(save_folder, exist_ok=True)
        
        try:
            model, config = load_config_and_model(exp_dir)
            feature_cols = config.get('feature_cols')
            
            # Carrega dados (Scalers v√™m da raiz agora)
            dataset, df_proc = load_data(
                CSV_PATH, 
                feature_cols, 
                config['input_seq_len'], 
                config['output_seq_len']
            )
            
            full_loader = DataLoader(dataset, batch_size=64, shuffle=False)
            xai = SolarXAIEngine(model, DEVICE)
            
            # AN√ÅLISE 1: Global
            print("   üìä [1/6] An√°lise Global de Vari√°veis...")
            global_imp = xai.compute_global_feature_importance(full_loader)
            xai.plot_global_importance(global_imp, dataset.data_input, save_path=os.path.join(save_folder, '1_Global_Importance.png'))
            
            # AN√ÅLISE 2: Temporal
            print("   ‚è≥ [2/6] An√°lise Temporal (Mem√≥ria do Modelo)...")
            temporal_map = xai.compute_temporal_importance(full_loader, target_step_idx=0, max_samples=500)
            xai.plot_temporal_profile(temporal_map, feature_cols, save_path=os.path.join(save_folder, '2_Temporal_Profile.png'))

            # AN√ÅLISE 3: F√≠sica
            print("   ‚òÅÔ∏è [3/6] An√°lise por Condi√ß√£o de C√©u...")
            if 'k' in df_proc.columns:
                # Mapeia indices do DataFrame para √≠ndices do Dataset
                # Pega √≠ndices onde k existe e √© v√°lido
                k_values = df_proc['k'].values
                indices_clear = np.where(k_values > 0.6)[0]
                indices_cloudy = np.where(k_values < 0.3)[0]
                valid_indices_arr = np.array(dataset.valid_indices)
                
                def get_subset_loader(indices_condition, max_s=200):
                    common_indices = np.intersect1d(indices_condition, valid_indices_arr)
                    if len(common_indices) == 0: return None
                    
                    if len(common_indices) > max_s: 
                        common_indices = np.random.choice(common_indices, max_s, replace=False)
                    
                    dataset_indices = []
                    for idx in common_indices:
                        pos = np.where(valid_indices_arr == idx)[0]
                        if len(pos) > 0: dataset_indices.append(pos[0])
                        
                    if not dataset_indices: return None
                    return DataLoader(Subset(dataset, dataset_indices), batch_size=32)

                loader_clear = get_subset_loader(indices_clear)
                loader_cloudy = get_subset_loader(indices_cloudy)
                
                if loader_clear:
                    map_clear = xai.compute_temporal_importance(loader_clear, target_step_idx=0)
                    xai.plot_heatmap(map_clear, feature_cols, "Import√¢ncia M√©dia - C√©u Claro", save_path=os.path.join(save_folder, '3a_Heatmap_Clear.png'))
                
                if loader_cloudy:
                    map_cloudy = xai.compute_temporal_importance(loader_cloudy, target_step_idx=0)
                    xai.plot_heatmap(map_cloudy, feature_cols, "Import√¢ncia M√©dia - C√©u Nublado", save_path=os.path.join(save_folder, '3b_Heatmap_Cloudy.png'))
            else:
                print("      ‚ö†Ô∏è Vari√°vel 'k' n√£o encontrada. Pulando an√°lise f√≠sica.")

            # AN√ÅLISE 4: Casos Extremos
            print("   üéØ [4/6] Estudo de Caso (Melhor vs Pior Erro)...")
            errors_list = []
            with torch.no_grad():
                for i, (x_batch, y_batch) in enumerate(full_loader):
                    x_batch = x_batch.to(DEVICE)
                    y_batch = y_batch.to(DEVICE)
                    preds = model(x_batch)
                    
                    p = preds[:, 0, 0].view(-1)
                    y = y_batch[:, 0].view(-1)
                    
                    batch_err = torch.abs(p - y).cpu().numpy()
                    errors_list.append(batch_err)
            
            if errors_list:
                errors = np.concatenate(errors_list)
                if len(errors) > 0:
                    idx_best_rel = np.argmin(errors)
                    idx_worst_rel = np.argmax(errors)
                    
                    x_best = dataset[idx_best_rel][0].unsqueeze(0)
                    map_best = xai.get_local_explanation(x_best)
                    xai.plot_heatmap(map_best, feature_cols, f"Melhor Caso (Erro: {errors[idx_best_rel]:.4f})", save_path=os.path.join(save_folder, '4a_Case_Best.png'))

                    x_worst = dataset[idx_worst_rel][0].unsqueeze(0)
                    map_worst = xai.get_local_explanation(x_worst)
                    xai.plot_heatmap(map_worst, feature_cols, f"Pior Caso (Erro: {errors[idx_worst_rel]:.4f})", save_path=os.path.join(save_folder, '4b_Case_Worst.png'))
                    
                    # AN√ÅLISE 5 e 6
                    if config.get('use_attention', False):
                        print("   üß† [5/6] Aten√ß√£o Temporal...")
                        attn_map = xai.collect_attention_maps(x_worst)
                        if attn_map is not None:
                            xai.plot_attention_map(attn_map, title="Temporal Attention - Pior Caso", save_path=os.path.join(save_folder, '5_Temporal_Attn_Worst.png'))

                    if config.get('use_feature_attention', False):
                        print("   üß¨ [6/6] Aten√ß√£o de Vari√°veis...")
                        feat_map = xai.collect_feature_weights(x_worst)
                        if feat_map is not None:
                            xai.plot_feature_weights(feat_map, feature_names=feature_cols, title="Feature Attention - Pior Caso", save_path=os.path.join(save_folder, '6_Feature_Attn_Worst.png'))

        except Exception as e:
            print(f"‚ùå Erro ao processar {model_name}: {e}")
            import traceback
            traceback.print_exc()

    print(f"\n‚úÖ An√°lise XAI Completa salva em: {OUTPUT_DIR}")

if __name__ == "__main__":
    main()